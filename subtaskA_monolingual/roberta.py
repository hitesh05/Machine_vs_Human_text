# -*- coding: utf-8 -*-
# """roberta_gpt2.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1k8AELfbxEjIwe9iHFamgGBVdM7H2zgC5
# """

# Dependencies
# !pip install datasets
# !pip install transformers
# !pip install transformers[torch]
# !pip install wandb

import pandas as pd
import datasets
from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
import wandb
import os

# from google.colab import drive
# drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

import os
os.environ["WANDB_DISABLED"] = "true"
seed = 25
# random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

train_data = pd.read_json('SemEval8/SubtaskA/subtaskA_train_monolingual.jsonl', lines=True)
train_data = train_data.reset_index(drop=True)
dev_data = pd.read_json('SemEval8/SubtaskA/subtaskA_dev_monolingual.jsonl', lines=True)
dev_data = dev_data.reset_index(drop=True)

model = RobertaForSequenceClassification.from_pretrained('roberta-base')
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)

import pandas as pd
from datasets import Dataset
def tokenization(batched_text):
    return tokenizer(batched_text['text'], padding = True, truncation=True)

train_dataset = Dataset.from_pandas(train_data)
dev_dataset = Dataset.from_pandas(dev_data)
train_dataset = train_dataset.map(tokenization, batched = True, batch_size = len(train_data))
dev_dataset = dev_dataset.map(tokenization, batched = True, batch_size = len(dev_data))

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
dev_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }



training_args = TrainingArguments(
    output_dir = 'SemEval8/SubtaskA/results',
    num_train_epochs=5,
    per_device_train_batch_size = 4,
    gradient_accumulation_steps = 16,
    per_device_eval_batch_size= 8,
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    disable_tqdm = False,
    load_best_model_at_end=True,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps = 8,
    fp16 = True,
    logging_dir='SemEval8/SubtaskA/logs',
    dataloader_num_workers = 8,
    run_name = 'roberta-classification'
)

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset
)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

trainer.train()

# DONT USE THIS YET
# from sklearn.model_selection import train_test_split
# train_texts = train_data['text'].to_list()[:6000]
# train_labels = train_data['label'].to_list()[:6000]

# val_texts = train_data['text'].to_list()[:2000]
# val_labels = train_data['label'].to_list()[:2000]

# print(len(train_texts))
# print(len(val_texts))

# !pip install transformers

# from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding
# from transformers import TrainingArguments, Trainer
# tokenizer = AutoTokenizer.from_pretrained("gpt2")
# tokenizer.pad_token = tokenizer.eos_token

# model = AutoModelForSequenceClassification.from_pretrained("gpt2").to(device)
# model.config.pad_token_id = model.config.eos_token_id
# print("Model Configurations")
# print()
# print(model.config)

# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)
# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)
# # test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)

# class CustomDataset(torch.utils.data.Dataset):
#     def __init__(self, encodings, labels):
#         self.encodings = encodings
#         self.labels = labels

#     def __getitem__(self, idx):
#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
#         if self.labels[idx] == 0:
#             item['labels'] = torch.tensor(0)
#         else:
#             item['labels'] = torch.tensor(1)
#         return item

#     def __len__(self):
#         return len(self.labels)

# train_dataset = CustomDataset(train_encodings, train_labels)
# val_dataset = CustomDataset(val_encodings, val_labels)
# # test_dataset = CustomDataset(test_encodings, test_labels)

# print("Sample Data Point")
# print()
# print(train_dataset[0])

# from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# def compute_metrics(eval_pred):
#     predictions, labels = eval_pred
#     predictions = np.argmax(predictions, axis=1)
#     precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')
#     acc = accuracy_score(labels, predictions)
#     return {
#         'accuracy': acc,
#         'f1': f1,
#         'precision': precision,
#         'recall': recall
#     }

# !pip install accelerate -U
# !pip install transformers[torch]

# training_args = TrainingArguments(
#     output_dir='./SubtaskA_gpt_models/english_gpt2_task1',
#     num_train_epochs=5,
#     per_device_train_batch_size=8,
#     per_device_eval_batch_size=8,
#     warmup_steps=500,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     logging_steps=10,
#     load_best_model_at_end=True,
#     metric_for_best_model='accuracy',
#     evaluation_strategy='epoch',
#     save_strategy='epoch',
#     save_total_limit=1,
# )
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     compute_metrics=compute_metrics
# )
# trainer.train()

# # trainer.evaluate(test_dataset)
# # predictions, labels, _ = trainer.predict(test_dataset)
# # predictions = np.argmax(predictions, axis=1)

# # from sklearn.metrics import classification_report
# # print(classification_report(labels, predictions))

# trainer.save_model('./models/english_gpt2_task1/trained_model')

