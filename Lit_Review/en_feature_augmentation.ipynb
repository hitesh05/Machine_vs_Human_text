{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ab5284-41b9-4d0f-b529-2fa7b29d9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0cc42b-5960-44a4-b0a0-4a251e5e532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a39071-e43f-46a6-893f-ce219f6dd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240511a7-0cd8-4abc-9d00-bf68a4ba7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 25\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724952d7-1d42-4f71-acfc-dbf71b58217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text      label\n",
      "0  12322  you need to stop the engine and wait until it ...  generated\n",
      "1   1682  The Commission shall publish the report; an in...  generated\n",
      "2  22592  I have not been tweeting a lot lately, but I d...  generated\n",
      "3  17390  I pass my exam and really thankgod for that bu...      human\n",
      "4  30453  The template will have 3 parts: a mustache sha...      human\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('datasets/subtask_1/en/train.tsv',sep='\\t')\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883429df-a252-41d8-9e6a-cf224bf9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size:  27414\n",
      "validation data size:  3046\n",
      "test data size:  3385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data_texts = train_data['text'].to_list()\n",
    "train_data_labels = train_data['label'].to_list()\n",
    "train_data_labels = [0 if x=='human' else 1 for x in train_data_labels]\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_data_texts, train_data_labels, test_size=0.1, random_state=25)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.1, random_state=25)\n",
    "print('train data size: ', len(train_texts))\n",
    "print('validation data size: ', len(val_texts))\n",
    "print('test data size: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246a21d5-d834-4052-8a29-7c1d4325e96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0351872444152832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69ffda82f7b469385bdf7e54e5d6ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025764942169189453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 28,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cf7be930964338b9081f83a942dd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.027325153350830078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 570,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d978b2d3003346aabdcfd75285da1317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02999091148376465,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 440473133,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a4d4268e544e2bb6f97e7b033d1236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configurations\n",
      "\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "print(\"Model Configurations\")\n",
    "print()\n",
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05431d70-74a3-49dc-b3e5-7cbf6b007c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 27414/27414 [04:28<00:00, 102.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train embeddings shape:  (27414, 768)\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    encoded_input = bert_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "    #get bert embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**encoded_input)\n",
    "    bert_embeddings = bert_output.last_hidden_state[:,0,:].cpu().numpy()\n",
    "    return bert_embeddings\n",
    "\n",
    "#get train embeddings\n",
    "train_embeddings = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_embeddings.append(get_bert_embeddings(text))\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "train_embeddings = np.squeeze(train_embeddings, axis=1)\n",
    "print('train embeddings shape: ', train_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7050edb3-44bd-4884-8ac3-53bb6ea4c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3046/3046 [00:29<00:00, 103.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation embeddings shape:  (3046, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3385/3385 [00:32<00:00, 103.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test embeddings shape:  (3385, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#get validation embeddings\n",
    "val_embeddings = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_embeddings.append(get_bert_embeddings(text))\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "val_embeddings = np.squeeze(val_embeddings, axis=1)\n",
    "print('validation embeddings shape: ', val_embeddings.shape) #shape: (num_samples, 1, 768)\n",
    "\n",
    "\n",
    "#get test embeddings\n",
    "test_embeddings = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_embeddings.append(get_bert_embeddings(text))\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_embeddings = np.squeeze(test_embeddings, axis=1)\n",
    "print('test embeddings shape: ', test_embeddings.shape) #shape: (num_samples, 1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9983d3f9-9d90-4452-aace-f8b6f436a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train punc shape:  (27414,)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def count_punctuations(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return count\n",
    "\n",
    "train_punc = []\n",
    "for text in train_texts:\n",
    "    train_punc.append(count_punctuations(text))\n",
    "train_punc = np.array(train_punc)\n",
    "\n",
    "val_punc = []\n",
    "for text in val_texts:\n",
    "    val_punc.append(count_punctuations(text))\n",
    "val_punc = np.array(val_punc)\n",
    "\n",
    "test_punc = []\n",
    "for text in test_texts:\n",
    "    test_punc.append(count_punctuations(text))\n",
    "test_punc = np.array(test_punc)\n",
    "print('train punc shape: ', train_punc.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06d55ee-e255-4047-b08e-e2cb194a37f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train capital shape:  (27414,)\n"
     ]
    }
   ],
   "source": [
    "def count_capital_letters(text):\n",
    "    count = sum([1 for char in text if char.isupper()])\n",
    "    return count\n",
    "\n",
    "train_capital = []\n",
    "for text in train_texts:\n",
    "    train_capital.append(count_capital_letters(text))\n",
    "train_capital = np.array(train_capital)\n",
    "\n",
    "val_capital = []\n",
    "for text in val_texts:\n",
    "    val_capital.append(count_capital_letters(text))\n",
    "val_capital = np.array(val_capital)\n",
    "\n",
    "test_capital = []\n",
    "for text in test_texts:\n",
    "    test_capital.append(count_capital_letters(text))\n",
    "test_capital = np.array(test_capital)\n",
    "print('train capital shape: ', train_capital.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab44401c-5ed6-4441-93f3-c9170c4edf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform sentiment analysis on a spanish text\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5380762c-118f-4560-a620-b78aaf0bc5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 27414/27414 [19:33<00:00, 23.36it/s]\n",
      "100%|███████████████████████████████████████| 3046/3046 [02:17<00:00, 22.20it/s]\n",
      "100%|███████████████████████████████████████| 3385/3385 [02:27<00:00, 22.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentiment shape:  (27414,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = sentiment_analysis(text)[0]['label']\n",
    "    #remove stars\n",
    "    #if its 1 star return 1\n",
    "    if sentiment == '1 star':\n",
    "        sentiment = 1\n",
    "    #if its 2 stars return 2\n",
    "    elif sentiment == '2 stars':\n",
    "        sentiment = 2\n",
    "    #if its 3 stars return 3\n",
    "    elif sentiment == '3 stars':\n",
    "        sentiment = 3\n",
    "    #if its 4 stars return 4\n",
    "    elif sentiment == '4 stars':\n",
    "        sentiment = 4\n",
    "    #if its 5 stars return 5\n",
    "    elif sentiment == '5 stars':\n",
    "        sentiment = 5\n",
    "    return sentiment #dim: (num_samples, 1) range: [1,5]\n",
    "\n",
    "train_sentiment = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_sentiment.append(get_sentiment(text))\n",
    "train_sentiment = np.array(train_sentiment)\n",
    "\n",
    "val_sentiment = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_sentiment.append(get_sentiment(text))\n",
    "val_sentiment = np.array(val_sentiment)\n",
    "\n",
    "test_sentiment = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_sentiment.append(get_sentiment(text))\n",
    "test_sentiment = np.array(test_sentiment)\n",
    "print('train sentiment shape: ', train_sentiment.shape) #shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a2ac049-cc43-4c2a-860d-3c09d742953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 27414/27414 [04:38<00:00, 98.50it/s]\n",
      "100%|███████████████████████████████████████| 3046/3046 [00:30<00:00, 99.48it/s]\n",
      "100%|███████████████████████████████████████| 3385/3385 [00:34<00:00, 99.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pos shape:  (27414, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#function to get pos tags for each category\n",
    "def get_pos(text):\n",
    "    doc = nlp(text)\n",
    "    adj_count = 0\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adp_count = 0\n",
    "    det_count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adj_count += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_count += 1\n",
    "        elif token.pos_ == 'VERB':\n",
    "            verb_count += 1\n",
    "        elif token.pos_ == 'ADP':\n",
    "            adp_count += 1\n",
    "        elif token.pos_ == 'DET':\n",
    "            det_count += 1\n",
    "    return [adj_count, noun_count, verb_count, adp_count, det_count] #dim: (num_samples, 5) \n",
    "\n",
    "train_pos = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_pos.append(get_pos(text))\n",
    "train_pos = np.array(train_pos)\n",
    "\n",
    "val_pos = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_pos.append(get_pos(text))\n",
    "val_pos = np.array(val_pos)\n",
    "\n",
    "test_pos = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_pos.append(get_pos(text))\n",
    "test_pos = np.array(test_pos)\n",
    "print('train pos shape: ', train_pos.shape) #shape: (num_samples, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67b74ff8-322c-4800-a2b1-960d2c618740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "ner_analysis = pipeline(\"ner\", model=\"balamurugan1603/bert-finetuned-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d1fe338-ffd1-438f-974e-853411dc875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 27414/27414 [20:16<00:00, 22.53it/s]\n",
      "100%|███████████████████████████████████████| 3046/3046 [02:17<00:00, 22.13it/s]\n",
      "100%|███████████████████████████████████████| 3385/3385 [02:30<00:00, 22.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ner shape:  (27414, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ner(text):\n",
    "    ner = ner_analysis(text)\n",
    "    loc_count = 0\n",
    "    org_count = 0\n",
    "    per_count = 0\n",
    "    misc_count = 0\n",
    "    for item in ner:\n",
    "        if item['entity'] == 'B-LOC' or item['entity'] == 'I-LOC':\n",
    "            loc_count += 1\n",
    "        elif item['entity'] == 'B-ORG' or item['entity'] == 'I-ORG':\n",
    "            org_count += 1\n",
    "        elif item['entity'] == 'B-PER' or item['entity'] == 'I-PER':\n",
    "            per_count += 1\n",
    "        elif item['entity'] == 'B-MISC' or item['entity'] == 'I-MISC':\n",
    "            misc_count += 1\n",
    "    return [loc_count, org_count, per_count, misc_count] #dim: (num_samples, 4)\n",
    "\n",
    "train_ner = []\n",
    "for text in tqdm(train_texts):\n",
    "    train_ner.append(get_ner(text))\n",
    "train_ner = np.array(train_ner)\n",
    "\n",
    "val_ner = []\n",
    "for text in tqdm(val_texts):\n",
    "    val_ner.append(get_ner(text))\n",
    "val_ner = np.array(val_ner)\n",
    "\n",
    "test_ner = []\n",
    "for text in tqdm(test_texts):\n",
    "    test_ner.append(get_ner(text))\n",
    "test_ner = np.array(test_ner)\n",
    "print('train ner shape: ', train_ner.shape) #shape: (num_samples, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c68d2aa-dac0-4eec-8ec8-43aacd347712",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_punc = train_punc.reshape(-1,1)\n",
    "val_punc = val_punc.reshape(-1,1)\n",
    "test_punc = test_punc.reshape(-1,1)\n",
    "\n",
    "train_capital = train_capital.reshape(-1,1)\n",
    "val_capital = val_capital.reshape(-1,1)\n",
    "test_capital = test_capital.reshape(-1,1)\n",
    "\n",
    "train_sentiment = train_sentiment.reshape(-1,1)\n",
    "val_sentiment = val_sentiment.reshape(-1,1)\n",
    "test_sentiment = test_sentiment.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d432e9b1-9d1c-4330-a260-c534b20357f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features shape:  (27414, 12)\n",
      "[ 7  6  2  7 24 10  7 12  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#concatenate all features\n",
    "train_features = np.concatenate((train_punc, train_capital,train_sentiment, train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "val_features = np.concatenate((val_punc, val_capital,val_sentiment, val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "test_features = np.concatenate((test_punc, test_capital,test_sentiment, test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "print(train_features[0])\n",
    "#concatenate all features\n",
    "# train_features = np.concatenate((train_pos, train_ner), axis=1) #dim: (num_samples, 11)\n",
    "# val_features = np.concatenate((val_pos, val_ner), axis=1) #dim: (num_samples, 11)\n",
    "# test_features = np.concatenate((test_pos, test_ner), axis=1) #dim: (num_samples, 11)\n",
    "# print('train features shape: ', train_features.shape) #shape: (num_samples, 11)\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c55bdc50-85cb-4bfd-b151-0df3602d29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save these features in a file\n",
    "np.save('en_train_features.npy', train_features)\n",
    "np.save('en_val_features.npy', val_features)\n",
    "np.save('en_test_features.npy', test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bcf71b-28cf-4227-8593-bc0946540a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60687fc5-48dd-4f13-a45f-5232dbf168e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# # Set the number of components you want to keep\n",
    "# n_components = 15\n",
    "# # Fit PCA on the validation embeddings and transform them\n",
    "# pca = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f722eac-0bc5-481f-aa16-be080b667bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings_pca = pca.fit_transform(train_embeddings)\n",
    "# print('train embeddings pca shape: ', train_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# val_embeddings_pca = pca.transform(val_embeddings)\n",
    "# print('validation embeddings pca shape: ', val_embeddings_pca.shape) #shape: (num_samples, n_components)\n",
    "\n",
    "# test_embeddings_pca = pca.transform(test_embeddings)\n",
    "# print('test embeddings pca shape: ', test_embeddings_pca.shape) #shape: (num_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47bab456-c4e9-4024-9269-0997bbc96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysentimiento import create_analyzer\n",
    "# analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "# text = \"Este es un ejemplo de texto con sentimiento.\"\n",
    "\n",
    "# result = analyzer.predict(text)\n",
    "\n",
    "# pos_prob = result.prob_pos\n",
    "# neg_prob = result.prob_neg\n",
    "# neu_prob = result.prob_neu\n",
    "\n",
    "# print(\"Positive Probability:\", pos_prob)\n",
    "# print(\"Negative Probability:\", neg_prob)\n",
    "# print(\"Neutral Probability:\", neu_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4c8fe8d-c6d0-4c9a-aa85-04ef212b95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 12])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_embeddings), torch.tensor(train_labels), torch.tensor(train_features))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(val_embeddings), torch.tensor(val_labels), torch.tensor(val_features))\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_embeddings), torch.tensor(test_labels), torch.tensor(test_features))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for embeddings, labels, f in train_loader:\n",
    "    print(embeddings.shape)\n",
    "    print(labels.shape)\n",
    "    print(f.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a656e10f-25e2-4afa-9299-96ef28cce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define neural network architecture\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# #create a neural network to use the embeddings and do classification\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_classes):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "#         self.dropout1 = nn.Dropout(0.1)\n",
    "#         self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # out = F.relu(self.bn1(self.fc1(x)))\n",
    "#         out = F.relu(self.fc1(x))\n",
    "#         out = self.dropout1(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "    \n",
    "# # Hyperparameters\n",
    "# input_size = 768\n",
    "# hidden_size = 128\n",
    "# num_classes = 2\n",
    "# num_epochs = 20\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77a3db57-1027-4a3b-91e0-1b8fb625f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#create a neural network to use the embeddings and do classification\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2+12, num_classes) #11 is the number of features\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, f):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = torch.cat((out, f), dim=1)    \n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "# Hyperparameters\n",
    "input_size = 768\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 28\n",
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16088ff5-22ca-4c06-8adc-dc296bca8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model from the neural network\n",
    "# model = Net(input_size, hidden_size, num_classes).to(device)\n",
    "model = Net(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9434cde1-daaf-495b-9bcc-436b37d8ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# best_val_acc = 0.0\n",
    "# total_step = len(train_loader)\n",
    "# half_epoch_step = total_step // 2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, (embeddings, labels) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "#         # Move tensors to the configured device\n",
    "#         embeddings = embeddings.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(embeddings)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Print loss every half epoch\n",
    "#         if (i+1) % half_epoch_step == 0:\n",
    "#             avg_loss = running_loss / half_epoch_step\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "#             running_loss = 0.0\n",
    "            \n",
    "#     # Validate the model\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for embeddings, labels in val_loader:\n",
    "#             embeddings = embeddings.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(embeddings)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         # Print validation stats\n",
    "#         val_acc = 100 * correct / total\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %')\n",
    "\n",
    "#         # Save the model if the validation accuracy is better than the previous best\n",
    "#         if val_acc > best_val_acc:\n",
    "#             best_val_acc = val_acc\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "#             print(f'Saved model with validation accuracy: {best_val_acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2692e33c-1f22-4548-8ef1-e16e1411ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  57%|██████████████▏          | 486/857 [00:01<00:00, 388.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 428 batches: 0.2585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 377.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Loss after 856 batches: 0.2524\n",
      "Epoch 1/20 Validation Accuracy: 85.33 %, Validation Loss: 0.3428\n",
      "Saved model with validation loss: 0.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  57%|██████████████▎          | 489/857 [00:01<00:00, 376.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 428 batches: 0.2414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 375.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Loss after 856 batches: 0.2532\n",
      "Epoch 2/20 Validation Accuracy: 84.90 %, Validation Loss: 0.3658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  55%|█████████████▋           | 469/857 [00:01<00:01, 387.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 428 batches: 0.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 388.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss after 856 batches: 0.2531\n",
      "Epoch 3/20 Validation Accuracy: 84.44 %, Validation Loss: 0.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  55%|█████████████▋           | 471/857 [00:01<00:00, 386.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 428 batches: 0.2438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 388.21batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Loss after 856 batches: 0.2402\n",
      "Epoch 4/20 Validation Accuracy: 85.49 %, Validation Loss: 0.3479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  55%|█████████████▋           | 471/857 [00:01<00:00, 389.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 428 batches: 0.2379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 389.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Loss after 856 batches: 0.2471\n",
      "Epoch 5/20 Validation Accuracy: 85.00 %, Validation Loss: 0.3612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  55%|█████████████▊           | 473/857 [00:01<00:00, 389.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 428 batches: 0.2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 388.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Loss after 856 batches: 0.2397\n",
      "Epoch 6/20 Validation Accuracy: 85.03 %, Validation Loss: 0.3663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  55%|█████████████▊           | 472/857 [00:01<00:00, 389.48batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 428 batches: 0.2365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 388.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Loss after 856 batches: 0.2301\n",
      "Epoch 7/20 Validation Accuracy: 85.49 %, Validation Loss: 0.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  55%|█████████████▊           | 472/857 [00:01<00:00, 389.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 428 batches: 0.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 388.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Loss after 856 batches: 0.2418\n",
      "Epoch 8/20 Validation Accuracy: 84.80 %, Validation Loss: 0.3405\n",
      "Saved model with validation loss: 0.3405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:  55%|█████████████▋           | 468/857 [00:01<00:01, 388.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 428 batches: 0.2235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████| 857/857 [00:02<00:00, 387.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Loss after 856 batches: 0.2269\n",
      "Epoch 9/20 Validation Accuracy: 85.16 %, Validation Loss: 0.3621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:  55%|█████████████▏          | 472/857 [00:01<00:00, 388.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 428 batches: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Loss after 856 batches: 0.2240\n",
      "Epoch 10/20 Validation Accuracy: 85.03 %, Validation Loss: 0.3678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:  55%|█████████████▏          | 470/857 [00:01<00:00, 388.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 428 batches: 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Loss after 856 batches: 0.2221\n",
      "Epoch 11/20 Validation Accuracy: 85.33 %, Validation Loss: 0.3665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:  55%|█████████████▏          | 470/857 [00:01<00:00, 388.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 428 batches: 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Loss after 856 batches: 0.2241\n",
      "Epoch 12/20 Validation Accuracy: 85.29 %, Validation Loss: 0.3740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  55%|█████████████▏          | 469/857 [00:01<00:00, 388.75batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 428 batches: 0.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Loss after 856 batches: 0.2162\n",
      "Epoch 13/20 Validation Accuracy: 86.34 %, Validation Loss: 0.3493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  55%|█████████████▎          | 474/857 [00:01<00:00, 389.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 428 batches: 0.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Loss after 856 batches: 0.2104\n",
      "Epoch 14/20 Validation Accuracy: 85.19 %, Validation Loss: 0.3593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:  55%|█████████████▏          | 472/857 [00:01<00:00, 389.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 428 batches: 0.2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Loss after 856 batches: 0.1999\n",
      "Epoch 15/20 Validation Accuracy: 85.23 %, Validation Loss: 0.3717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  55%|█████████████▎          | 475/857 [00:01<00:00, 389.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 428 batches: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████| 857/857 [00:02<00:00, 389.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Loss after 856 batches: 0.2189\n",
      "Epoch 16/20 Validation Accuracy: 84.93 %, Validation Loss: 0.3839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  55%|█████████████▏          | 469/857 [00:01<00:00, 389.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 428 batches: 0.2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████| 857/857 [00:02<00:00, 389.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Loss after 856 batches: 0.2094\n",
      "Epoch 17/20 Validation Accuracy: 85.16 %, Validation Loss: 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  55%|█████████████▏          | 469/857 [00:01<00:01, 387.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 428 batches: 0.1934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Loss after 856 batches: 0.2079\n",
      "Epoch 18/20 Validation Accuracy: 84.80 %, Validation Loss: 0.3773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  55%|█████████████▎          | 474/857 [00:01<00:00, 389.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 428 batches: 0.1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████| 857/857 [00:02<00:00, 388.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Loss after 856 batches: 0.2057\n",
      "Epoch 19/20 Validation Accuracy: 84.96 %, Validation Loss: 0.3854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  55%|█████████████▎          | 475/857 [00:01<00:00, 389.61batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 428 batches: 0.2055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████| 857/857 [00:02<00:00, 389.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Loss after 856 batches: 0.2012\n",
      "Epoch 20/20 Validation Accuracy: 85.72 %, Validation Loss: 0.3801\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf') # initialize best validation loss to infinity\n",
    "total_step = len(train_loader)\n",
    "half_epoch_step = total_step // 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (embeddings, labels, f) in tqdm(enumerate(train_loader), total=total_step, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "        # Move tensors to the configured device\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, f)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print loss every half epoch\n",
    "        if (i+1) % half_epoch_step == 0:\n",
    "            avg_loss = running_loss / half_epoch_step\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} Loss after {i+1} batches: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    # Validate the model\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for embeddings, labels, f in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "            f = f.to(device)\n",
    "            outputs = model(embeddings, f)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print validation stats\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} Validation Accuracy: {val_acc:.2f} %, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save the model if the validation loss is better than the previous best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Saved model with validation loss: {best_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d92a668c-3515-4613-a91c-34f71d9a39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 106/106 [00:00<00:00, 801.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    for embeddings, labels, f in tqdm(test_loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        f = f.to(device)\n",
    "        outputs = model(embeddings,f)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    #generate classification report\n",
    "    test_report = classification_report(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d2bd8f0-7b27-4d05-857f-448beba8089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      1703\n",
      "           1       0.84      0.85      0.85      1682\n",
      "\n",
      "    accuracy                           0.85      3385\n",
      "   macro avg       0.85      0.85      0.85      3385\n",
      "weighted avg       0.85      0.85      0.85      3385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "pyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
