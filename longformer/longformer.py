# -*- coding: utf-8 -*-
"""longoformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ur80Mm6G8rg9SIuM8veGcAPkEInlep4W
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Dependencies
# !pip install transformers[torch]
# !pip install transformers
# !pip install datasets
# !pip install wandb

import pandas as pd
import datasets
from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
import wandb
import os

config = LongformerConfig()
print(config)

train_data = pd.read_json('/content/drive/MyDrive/SemEval8/SubtaskA/subtaskA_train_monolingual.jsonl', lines=True)
train_data = train_data.reset_index(drop=True)[:7000]
dev_data = pd.read_json('/content/drive/MyDrive/SemEval8/SubtaskA/subtaskA_dev_monolingual.jsonl', lines=True)
dev_data = dev_data.reset_index(drop=True)[:1000]

import pandas as pd
from datasets import Dataset

train_dataset = Dataset.from_pandas(train_data)
dev_dataset = Dataset.from_pandas(dev_data)

model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',
                                                           gradient_checkpointing=False,
                                                           attention_window = 512)
tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 4096)

model.config

def tokenization(batched_text):
    return tokenizer(batched_text['text'], padding = 'max_length', truncation=True, max_length = 4096)

train_dataset = train_dataset.map(tokenization, batched = True, batch_size = len(train_dataset))
dev_dataset = dev_dataset.map(tokenization, batched = True, batch_size = len(dev_dataset))

len(train_dataset['input_ids'][0])

# LongformerForSequenceClassification
# implementation by default sets the global attention to the CLS token,
# so there is no need to further modify the inputs.

# define the training arguments
training_args = TrainingArguments(
    output_dir = 'longformer_ANLP_outputs/',
    num_train_epochs = 1,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 8,
    per_device_eval_batch_size= 1,
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    disable_tqdm = False,
    load_best_model_at_end=True,
    warmup_steps=200,
    weight_decay=0.01,
    logging_steps = 4,
    fp16 = True,
    logging_dir='longformer_ANLP_outputs/',
    dataloader_num_workers = 0,
    run_name = 'longformer-SemEval8_SubtaskA_mono'
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset
)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

trainer.train()

trainer.save_model('longformer_ANLP_outputs/')